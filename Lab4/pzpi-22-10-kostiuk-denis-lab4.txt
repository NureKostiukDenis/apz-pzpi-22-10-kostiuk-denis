Харківський національний університет радіоелектроніки

Кафедра програмної інженерії






Звіт
З Лабораторної роботи № 4
з дисципліни «Архітектура програмного забезпечення» 





Виконав:                
ст. гр. ПЗПІ 22-10  Костюк Д.В.

Перевірив: 
асистент каф. ПІ Дашенков  Д. С.







Харків 2025


     
     Посилання на гіт: https://github.com/NureKostiukDenis/apz-pzpi-22-10-kostiuk-denis/tree/main/Lab4/pzpi-22-10-kostiuk-denis-lab4
     
1. Мета роботи 

  У межах цієї лабораторної роботи було обрано горизонтальне масштабування монолітного бекенд-сервісу, розробленого на Spring Boot.
  Мета: забезпечити обробку зростаючої кількості одночасних користувачів без деградації продуктивності.
  
2. Інженерні рішення 

• Minikube для створення локальних кластерів для балансування
   • Контейнеризація застосунку за допомогою Docker. Бекенд-сервіс було зібрано у Docker-образ anware_server, який локально імпортувався до Minikube для подальшої роботи.
   • Розгортання застосунку у Kubernetes-кластері (через Minikube).
   • Масштабування здійснюється шляхом збільшення кількості подів spring-app, що запускають однаковий екземпляру серверного застосунку.
   • Балансування запитів між подами забезпечується через Kubernetes Service (spring-service). При цьому кількість реплік змінювалася командою kubectl scale deployment spring-app --replicas=3
     
     
     Рисунок 1 – приклад балансування між 5 екземплярами основного серверу
     
     
     Рисунок 2 – приклад балансування між 3 екземплярами основного серверу
     
     
     Рисунок 3 – 1 екземпляр основного серверу
     
   • Locust використано як інструмент для створення навантаження і спостереження за впливом масштабування. 
     
3. Під час тестування було виявлено такі вузькі місця:
     CPU — першим обмеженням при зростанні навантаження став саме процесор, оскільки кожен под опрацьовує запити незалежно.
     База даних — оскільки база не була розділена або масштабована, її продуктивність стає критичною при великій кількості одночасних запитів. Очікуване зменшення навантаження на окремий екземпляр при масштабуванні не призводить до суттєвого покращення часу відповіді. Це свідчить, що не бекенд є вузьким місцем, а саме база даних. Це наглядно можна побачити в таблиці 1.
     
     Таблиця №1 – метрики серверу 
Кількість екземплярів
Кількість користувачів
Час відповіді, мс
Запитів на секунду 
1
200
21
98
3
200
23
99
5
200
28
100
     
     Масштабування бекенда дозволяє збільшити кількість запитів, які система може обробити в одиницю часу, однак через обмеження з боку бази даних цей приріст є нелінійним. Це вказує на потребу в подальшій оптимізації СУБД або впровадженні кешування.

     Для перевірки стійкості системи при значному навантаженні було проведено тестування із використанням 500 віртуальних користувачів. Було перевірено поведінку системи при різній кількості серверних екземплярів.
     На рисунку 4 показано результати тестування для 3 екземплярів серверу. В процесі виконання навантаження спостерігалася значна кількість помилок — сервер починав повертати відмови у з’єднанні або помилки таймауту. Це свідчить про те, що система у поточній конфігурації не справляється з обробкою такої кількості паралельних запитів, навіть за наявності горизонтального масштабування.
     

Рисунок 4 – метрики тесту серверу

     Для порівняння було виконано аналогічний тест із тією ж кількістю користувачів (500), але вже з одним екземпляром сервера. Результати наведено на рисунку 5.
     

Рисунок 5 – метрики тесту серверу

     На основі отриманих метрик було складено таблицю 2, що відображає ключові показники навантаження:
     
     Таблиця 2 – перевірка на високому навантаженні 
Кількість екземплярів
Кількість користувачів
Час відповіді, мс
Запитів на секунду 
Відсоток помилок від серверу
1
450
2000
180
12%
3
450
2000
182
15%


     Як видно з таблиці, збільшення кількості реплік не призвело до покращення показників продуктивності. Навпаки, частота помилок навіть зросла, що може свідчити про:
   • Обмеження на стороні бази даних, яка не масштабується разом з бекендом і стає точкою відмови при зростанні паралельних запитів;
   • Виснаження пулу з'єднань до СУБД, коли кожна репліка намагається встановити одночасні сесії;
   • Нестачу ресурсів (CPU, RAM) на вузлі Minikube, через що контейнери конкурують за ресурси.

     Таким чином, для ефективного масштабування потрібно не лише масштабувати бекенд, але й забезпечити масштабовану архітектуру бази даних або реалізувати кешування.

4. Висновок
     Горизонтальне масштабування бекенду дійсно дозволяє обробляти більше паралельних запитів, однак приріст продуктивності є нелінійним.
     Вузькі місця:
     • Процесор: кожен под запускає окрему JVM, що створює додаткове навантаження на CPU.
     • СУБД: оскільки всі поди звертаються до єдиного екземпляра бази даних, виникає конкуренція за її ресурси, що обмежує масштабованість всієї системи.
     • У деяких випадках збільшення кількості реплік навіть погіршувало час відповіді, що свідчить про перевантаження СУБД при одночасному зростанні кількості паралельних з'єднань.
     Таким чином, подальше масштабування бекенду потребує або масштабування СУБД (наприклад, за допомогою реплікації або шардингу), або впровадження кешування для зниження навантаження на базу.
     
